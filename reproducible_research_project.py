# -*- coding: utf-8 -*-
"""Reproducible Research Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/orkhan-amrullayev/7d878fd1b167f22fe6545f89a0ab1afe/reproducible-research-project.ipynb

edited by Orkhan and now Heshan also made changes 
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn.preprocessing import LabelEncoder,StandardScaler
from sklearn.model_selection import train_test_split
from random import random
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
from matplotlib import pyplot



#Importing the libraries 
import pandas as pd

df = pd.read_csv('/content/telco_churn (1).csv')

from google.colab import drive
drive.mount('/content/drive')

#Importing the datasetr
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/telco_churn.csv")

df.head()

df.describe()

# Find null values in the dataset
df.isnull().any(axis=1)

# Find null values in the dataset
df.dropna(axis = 1, how ='any', thresh = None, subset = None, inplace=False)
df.dropna(axis = 0, how ='any', thresh = None, subset = None, inplace=False)

# Find null values in the dataset
df.isnull().any(axis=0)

"""# **Analyzing the categorical features**"""

categorical_features=[feature for feature in df.columns if ((df[feature].dtypes=='O') )]
plt.figure(figsize=(40,280), facecolor='white')
plotnumber =1
for categorical_feature in categorical_features:
    ax = plt.subplot(15,3,plotnumber)
    sns.countplot(y=categorical_feature,data=df)
    plt.xlabel(categorical_feature)
    plt.title(categorical_feature)
    plotnumber+=1
plt.show()





"""# Findings


"""

df['Churn Value'].value_counts()

df[df['Churn Value'] == 0.0].sample(1869)





"""<h3>Correlation matrix</h3>"""

fig, ax = plt.subplots(figsize=(20,20)
sns.heatmap(df.corr(), annot=True, linewidths=.5, ax=ax)

"""Outcome of Correlation Matrix

# Check the Data set is balanced or not based on target values in classification
"""





"""**Integer encoding**"""

#Identify the categorical columns in the dataset
obj_df = df.select_dtypes(include=['object']).copy()
obj_df.head()

#Encoding the categorical values ( Integer encoding )
new_df = df.apply(LabelEncoder().fit_transform)
new_df.head()

#Randomization of the dataset
np.random.seed(1000)

# Seperating the Features and Target Columns 
df_feat = new_df[new_df.columns[0:-1]] # Feature columns 
df_head = new_df[new_df.columns[len(new_df.columns)-1]]  # Target variable

"""## Data Normalization"""

# Feature Scalling using Sklearn StandardScaler function
scaler = StandardScaler()
StandardScaler(copy=True,with_mean=True,with_std=True)
scaler.fit(df_feat)
scaled_features = scaler.transform(df_feat)
df_scaled = pd.DataFrame(scaled_features,columns=df_feat.columns)
df_scaled.head()

df_scaled.dtypes


"""## Feature Importance Analysis"""

##Feature Selection using XGBClassifier
model = XGBClassifier()
# fit the model
model.fit(df_scaled,df_head)
# get importance
importance = model.feature_importances_
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()

		       
# Synthetic Minority Oversampling Technique (SMOTE)
# SMOTE is used to remove the imbalance in the training data by creating samples using the current data. 
# It doesn't create any duplication. After perform this action, we have a balance training data.

from imblearn.over_sampling import SMOTE

smt = SMOTE()

x_train, y_train = smt.fit_sample(x_train, y_train)
np.bincount(y_train)
